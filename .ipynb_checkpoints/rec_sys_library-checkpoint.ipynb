{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit\n",
    "\n",
    "- implicit 데이터에 사용가능한 라이브러리\n",
    "\n",
    "- 가장 널리 사용된다.\n",
    "\n",
    "#### ALS Collaborative filtering\n",
    "\n",
    "- MF모델. ALS(AlternatingLeastSquares, 교대최대제곱법) 모델.\n",
    "\n",
    "- 유저행렬과 아이템 행렬을 차례대로 최적화\n",
    "\n",
    "#### Baysian Personalized Filtering\n",
    "\n",
    "- \"BPR: Bayesian Personalized Ranking from Implicit Feedback.\" 논문의 알고리즘.\n",
    "\n",
    "- 기존의 pointwise 기반의 optimization이 아닌 ranking optimization을 적용함. 논문에서는 MF, neighborhood model 모두 적용한다. 여기선 MF만 구현함. [논문](https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf)\n",
    "\n",
    "#### Logistic Matrix Factorization\n",
    "- 2014년 Spotify에서 발표한 논문\n",
    "\n",
    "- LMF는 MF에 logistic함수를 도입하여 item에 대한 user의 선호를 확률적으로 모델링한 것. [논문](https://web.stanford.edu/~rezab/nips2014workshop/submits/logmat.pdf)\n",
    "  \n",
    "  \n",
    "\n",
    "****\n",
    "# Surprise\n",
    "\n",
    "- explicit 데이터에 사용가능한 라이브러리\n",
    "\n",
    "## Basic algorithms\n",
    "\n",
    "#### NormalPredictor\n",
    "\n",
    "MLE를 통해서 추정한 N(μ^,σ^2)분포를 통해 예측\n",
    "\n",
    "- $\\begin{split}\\hat{\\mu} &= \\frac{1}{|R_{train}|} \\sum_{r_{ui} \\in R_{train}}\n",
    "r_{ui}\\hat{\\sigma} &= \\sqrt{\\sum_{r_{ui} \\in R_{train}}\n",
    "\\frac{(r_{ui} - \\hat{\\mu})^2}{|R_{train}|}}\\end{split}$\n",
    "\n",
    "\n",
    "#### BaselineOnly\n",
    "\n",
    "- $\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$\n",
    "\n",
    "user, item 평균치 고려\n",
    "\n",
    "## k-NN inspired algorithms\n",
    "\n",
    "#### KNNBasic\n",
    "\n",
    "- $\\hat{r}_{ui} = \\frac{\n",
    "\\sum\\limits_{v \\in N^k_i(u)} \\text{sim}(u, v) \\cdot r_{vi}}\n",
    "{\\sum\\limits_{v \\in N^k_i(u)} \\text{sim}(u, v)}$\n",
    "\n",
    "- $\\hat{r}_{ui} = \\frac{\n",
    "\\sum\\limits_{j \\in N^k_u(i)} \\text{sim}(i, j) \\cdot r_{uj}}\n",
    "{\\sum\\limits_{j \\in N^k_u(i)} \\text{sim}(i, j)}$\n",
    "\n",
    "- basic한 neighborhood model.\n",
    "- item based, user based 모두 가능\n",
    "\n",
    "#### KNNWithMeans\n",
    "\n",
    "- $\\hat{r}_{ui} = \\mu_u + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
    "\\text{sim}(u, v) \\cdot (r_{vi} - \\mu_v)} {\\sum\\limits_{v \\in\n",
    "N^k_i(u)} \\text{sim}(u, v)}$\n",
    "\n",
    "- $\\hat{r}_{ui} = \\mu_i + \\frac{ \\sum\\limits_{j \\in N^k_u(i)}\n",
    "\\text{sim}(i, j) \\cdot (r_{uj} - \\mu_j)} {\\sum\\limits_{j \\in\n",
    "N^k_u(i)} \\text{sim}(i, j)}$\n",
    "\n",
    "- 각 유저나 아이템의 mean 고려\n",
    "- basic한 neighborhood model.\n",
    "- item based, user based 모두 가능\n",
    "\n",
    "#### KNNWithZScore\n",
    "\n",
    "- $\\hat{r}_{ui} = \\mu_u + \\sigma_u \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
    "\\text{sim}(u, v) \\cdot (r_{vi} - \\mu_v) / \\sigma_v} {\\sum\\limits_{v\n",
    "\\in N^k_i(u)} \\text{sim}(u, v)}$\n",
    "\n",
    "- $\\hat{r}_{ui} = \\mu_i + \\sigma_i \\frac{ \\sum\\limits_{j \\in N^k_u(i)}\n",
    "\\text{sim}(i, j) \\cdot (r_{uj} - \\mu_j) / \\sigma_j} {\\sum\\limits_{j\n",
    "\\in N^k_u(i)} \\text{sim}(i, j)}$\n",
    "\n",
    "- 각 유저나 아이템의 mean, std 고려\n",
    "- basic한 neighborhood model.\n",
    "- item based, user based 모두 가능\n",
    "\n",
    "#### KNNBaseline\n",
    "\n",
    "- $\\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
    "\\text{sim}(u, v) \\cdot (r_{vi} - b_{vi})} {\\sum\\limits_{v \\in\n",
    "N^k_i(u)} \\text{sim}(u, v)}$\n",
    "\n",
    "- $\\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{j \\in N^k_u(i)}\n",
    "\\text{sim}(i, j) \\cdot (r_{uj} - b_{uj})} {\\sum\\limits_{j \\in\n",
    "N^k_u(i)} \\text{sim}(i, j)}$\n",
    "\n",
    "- 유저와 아이템의 bias 모두 고려\n",
    "- basic한 neighborhood model.\n",
    "- item based, user based 모두 가능\n",
    "\n",
    "\n",
    "\n",
    "## Matrix Factorization(SVD, SVD++, NMF, PMF)\n",
    "\n",
    "\n",
    "#### SVD\n",
    "- bias가 있는 기본적인 SVD 모델. \n",
    "\n",
    "- 규제항이 있는 squared error를 SGD 한다.  \n",
    "\n",
    "$\\hat{r}_{ui} = \\mu + b_u + b_i + q_i^Tp_u$\n",
    "\n",
    "\n",
    "$\\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
    "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2\\right)$\n",
    "\n",
    "biased = False할 시 bias없는 모델 사용가능\n",
    "\n",
    "#### SVD++\n",
    "\n",
    "- $\\hat{r}_{ui} = \\mu + b_u + b_i + q_i^T\\left(p_u +\n",
    "|I_u|^{-\\frac{1}{2}} \\sum_{j \\in I_u}y_j\\right)$\n",
    "\n",
    "implicit data 사용. SVD와 같이 규제항이 있는 squared error를 SGD 한다.\n",
    "\n",
    "### NMF\n",
    "\n",
    "- 음수 미포함 행렬 분해(Non-negative Matrix Factorization, NMF)는 음수를 포함하지 않는 행렬 X를 음수를 포함하지 않는 행렬 W와 H의 곱으로 분해하는 알고리즘\n",
    "\n",
    "- non-negative 데이터는 non-negative feature로 설명하는 것이 좋다는 점, feature들의 독립성을 잘 catch 할 수 있다는 점이 장점 [참고](https://angeloyeo.github.io/2020/10/15/NMF.html#%EC%99%9C-nmf%EB%A5%BC-%EC%93%B0%EB%8A%94-%EA%B2%83%EC%9D%B4-%EC%9C%A0%EC%9A%A9%ED%95%A0-%EC%88%98-%EC%9E%88%EC%9D%84%EA%B9%8C)\n",
    "\n",
    "- $\\hat{r}_{ui} = q_i^Tp_u$\n",
    "\n",
    "- $\\begin{split}p_{uf} &\\leftarrow p_{uf} &\\cdot \\frac{\\sum_{i \\in I_u} q_{if}\n",
    "\\cdot r_{ui}}{\\sum_{i \\in I_u} q_{if} \\cdot \\hat{r_{ui}} +\n",
    "\\lambda_u |I_u| p_{uf}}\\\\\n",
    "q_{if} &\\leftarrow q_{if} &\\cdot \\frac{\\sum_{u \\in U_i} p_{uf}\n",
    "\\cdot r_{ui}}{\\sum_{u \\in U_i} p_{uf} \\cdot \\hat{r_{ui}} +\n",
    "\\lambda_i |U_i| q_{if}}\\\\\\end{split}$\n",
    "\n",
    "***\n",
    "### LightFM\n",
    "\n",
    "- implicit, explicit 데이터 모두 사용가능.\n",
    "\n",
    "- LightFM 모델 구현가능\n",
    "\n",
    "- LightFM은 Collaborative filtering, Content based filtering이 결합된 hybrid model\n",
    "\n",
    "$q_u = \\sum_{j \\in f_u}e_j^U$    $p_i = \\sum_{j \\in f_i}e_j^I$ \n",
    "\n",
    "\n",
    "$b_u = \\sum_{j \\in f_u}b_j^U$     $b_i = \\sum_{j \\in f_i}b_j^I$ \n",
    " \n",
    "\n",
    "$\\hat{r}_{ui} = sigmoid(q_u \\odot p_i + b_u + b_i)$\n",
    "\n",
    "ex) 아이템이 데님자켓이라면 '데님'과 '자켓'의 latent vector들의 합이 $q_u$, 유저가 미국에 살고 여성이라면 둘의 latent vector의 합이 $p_i$\n",
    "\n",
    "유저와 아이템 각각의 feature와 둘 간의 상호작용을 모두 고려할 수 있다.\n",
    "\n",
    "cold start 문제를 완화할 수 있다.\n",
    "\n",
    "예측 평점이 아웃풋이 아닌 추천 점수가 아웃풋이다. 해당 유저가 해당 아이템을 선호할 확률같은 개념이라고 보면 될 듯하다.\n",
    "\n",
    "![img](https://greeksharifa.github.io/public/img/Machine_Learning/2020-06-01-LightFM/01.JPG)\n",
    "\n",
    "***\n",
    "### Buffalo\n",
    "\n",
    "- 다른 라이브러리에 비해서 적은 메모리 사용과 빠른 속도를 가짐.\n",
    "- 아래는 ALS 모델에 대한 비교로, Implicit 라이브러리 보다 좋은 성능을 보여준다.\n",
    "\n",
    "<img src=\"https://github.com/kakao/buffalo/raw/master/benchmark/fig/20190828.buffalo.kakaobrunch12m.d.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "\n",
    "- Alternating Least Squares\n",
    "- Bayesian Personalized Ranking Matrix Factorization\n",
    "- Word2Vec\n",
    "- CoFactors \n",
    "\n",
    "위의 알고리즘들이 구현되어 있다.\n",
    "\n",
    "***\n",
    "### Spotlight\n",
    "\n",
    "- implicit , explicit 데이터 사용가능\n",
    "\n",
    "- Factorization models(implicit/explicit), Sequantial models(implicit) 알고리즘이 구현되어 있음\n",
    "\n",
    "- sequaltial model을 사용가능한 것이 특징\n",
    "\n",
    "\n",
    "#### Factorization model\n",
    "\n",
    "- \"dot product of the item and user latent vectors koren's classic matrix factorization\" 라고 설명하는 것을 보아 SVD알고리즘인 듯 하다. \n",
    "\n",
    "$\\hat{r}_{ui} = \\mu + b_u + b_i + q_i^Tp_u$\n",
    "\n",
    "\n",
    "$\\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
    "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2\\right)$\n",
    "\n",
    "\n",
    "#### Sequaltial model \n",
    "\n",
    "- \"Deep Neural Networks for YouTube Recommendations\"의 pooling model,\n",
    "\n",
    "- \"SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS\"의 LSTM model,\n",
    "\n",
    "- WaveNet의 causal convolution model \n",
    "\n",
    "위의 3가지 종류의 모델 존재\n",
    "\n",
    "||Implicit|Surprise|LightFM|Buffalo|Spotlight|\n",
    "|------|:---:|:---:|:---:|:---:|:---:|\n",
    "|특징|가장 유명함. <br/>implicit만 가능. | explicit만 가능 |LightFM 구현가능 | 빠른 속도 | Sequantial 모델 구현가능|\n",
    "|implicit data|O|X|O|O|O|\n",
    "|explicit data|X|O|O|O|O|\n",
    "|Matrix Factorization|ALS, BPR, logistic MF|SVD, SVD++, NMF|LightFM|ALS, BPR|SVD|\n",
    "|neighborhood methods| Item-Item Nearest Neighbour models |KNNBasic, KNNwithMeans, KNNwithZscore, KNNBaseline|X |X |X |\n",
    "|Sequaltial model| X |X |X |X |O |\n",
    "|LightFM| X|X |O| X|X|\n",
    "\n",
    "\n",
    "참고자료\n",
    "\n",
    "https://leehyejin91.github.io/post-bpr/\n",
    "\n",
    "https://leehyejin91.github.io/post-logistic_mf/\n",
    "\n",
    "https://greeksharifa.github.io/machine_learning/2020/06/01/LightFM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
